{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf9e2d9",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Ensemble methods combines many many simple models, probably less powerful models, in order to obtain a single and potentially very powerful model.<br>\n",
    "Instead of training different model architectures, bagging uses bootstrapping to randomly (and repeately) sampling subsets of dataset and train multiple models with same architecture. Same effect of ensemble is obtained in this manner.<br>\n",
    "Random forests are bagging but with decision trees.<br><br>\n",
    "In bootstrapping, random data points (about 75%) from original dataset are sampled and duplicates are allowed. Resulting bootstrapped dataset is the same size as original dataset but with duplicates. Each tree is trained on each bootstrap dataset.<br>\n",
    "In bootstrapping, only a subset of dataset are used so there are unseen samples for each bootstrap dataset. These samples are called out-of-bag oob samples. oob samples are used to evaluate each tree to get oob score.<br><br>\n",
    "Decision trees have very high variance. Bagging reduces variance while retaining the same bias.<br>\n",
    "At each split, random forest uses only a subset of features to grow a tree. By doing so, different trees can have different behaviors depending on the features they see at each timestep, promoting diversity among trees. It can prevent one strong feature to be dominant in all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4847fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103594, 22) (103594,)\n",
      "(25893, 22) (25893,)\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# drop unneccessary columns\n",
    "df_train = df_train.drop(columns=['Unnamed: 0', 'id'])\n",
    "df_test = df_test.drop(columns=['Unnamed: 0', 'id'])\n",
    "\n",
    "# drop missing values\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# convert Arrival Delay in Minutes to integer\n",
    "df_train['Arrival Delay in Minutes'] = df_train['Arrival Delay in Minutes'].astype(np.int64)\n",
    "df_test['Arrival Delay in Minutes'] = df_test['Arrival Delay in Minutes'].astype(np.int64)\n",
    "\n",
    "# divide categorical and numerical features\n",
    "cat_feats = []\n",
    "num_feats = []\n",
    "\n",
    "for col, dtype in dict(df_train.dtypes).items():\n",
    "    if dtype == np.dtypes.ObjectDType:\n",
    "        cat_feats.append(col)\n",
    "    elif dtype == np.int64:\n",
    "        num_feats.append(col)\n",
    "\n",
    "target = 'satisfaction'\n",
    "cat_feats.remove(target)\n",
    "\n",
    "# prepare train and test sets\n",
    "train_features = cat_feats + num_feats\n",
    "\n",
    "for cf in cat_feats+[target]:\n",
    "    cats = list(df_train[cf].unique())\n",
    "    cats.sort()\n",
    "    encoder = OrdinalEncoder(categories=[cats])\n",
    "    df_train[cf] = encoder.fit_transform(df_train[[cf]]).astype(int)\n",
    "    \n",
    "    cats = list(df_test[cf].unique())\n",
    "    cats.sort()\n",
    "    encoder = OrdinalEncoder(categories=[cats])\n",
    "    df_test[cf] = encoder.fit_transform(df_test[[cf]]).astype(int)\n",
    "       \n",
    "X_train = df_train[train_features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_test = df_test[train_features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9999903469312894\n",
      "\n",
      "test accuracy: 0.9631174448692696\n",
      "\n",
      "confusion matrix:\n",
      "[[14236   292]\n",
      " [  663 10702]]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "\n",
    "print(f\"train accuracy: {accuracy}\\n\")\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"test accuracy: {accuracy}\\n\")\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f5d8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree(n_estimators, bootstrap):\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, bootstrap=bootstrap, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_pred)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfbeb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 80, bootstrap: True -> train: 0.9999903469312894, test: 0.9622677943845827\n",
      "n_estimators: 80, bootstrap: False -> train: 1.0, test: 0.9626153786737728\n",
      "n_estimators: 90, bootstrap: True -> train: 0.9999806938625789, test: 0.9626926196269262\n",
      "n_estimators: 90, bootstrap: False -> train: 1.0, test: 0.9630402039161163\n",
      "n_estimators: 100, bootstrap: True -> train: 0.9999903469312894, test: 0.9631174448692696\n",
      "n_estimators: 100, bootstrap: False -> train: 1.0, test: 0.9631174448692696\n",
      "n_estimators: 120, bootstrap: True -> train: 1.0, test: 0.9629629629629629\n",
      "n_estimators: 120, bootstrap: False -> train: 1.0, test: 0.9630015834395397\n",
      "n_estimators: 150, bootstrap: True -> train: 1.0, test: 0.9631174448692696\n",
      "n_estimators: 150, bootstrap: False -> train: 1.0, test: 0.9630015834395397\n",
      "n_estimators: 200, bootstrap: True -> train: 1.0, test: 0.9629243424863863\n",
      "n_estimators: 200, bootstrap: False -> train: 1.0, test: 0.9633877882053065\n",
      "n_estimators: 250, bootstrap: True -> train: 1.0, test: 0.9633877882053065\n",
      "n_estimators: 250, bootstrap: False -> train: 1.0, test: 0.9633877882053065\n",
      "n_estimators: 300, bootstrap: True -> train: 1.0, test: 0.9631560653458464\n",
      "n_estimators: 300, bootstrap: False -> train: 1.0, test: 0.9633491677287298\n",
      "\n",
      "best -> n_estimators: 200, bootstrap: False -> test acc: 0.9633877882053065\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [80, 90, 100, 120, 150, 200, 250, 300]\n",
    "bootstrap = [True, False]\n",
    "test_accs = []\n",
    "\n",
    "for ne in n_estimators:\n",
    "    for bo in bootstrap:\n",
    "        train_acc, test_acc = fit_tree(ne, bo)\n",
    "        print(f\"n_estimators: {ne}, bootstrap: {bo} -> train: {train_acc}, test: {test_acc}\")\n",
    "        test_accs.append((ne, bo, test_acc))\n",
    "\n",
    "best = max(test_accs, key=lambda x: x[-1])\n",
    "print(f\"\\nbest -> n_estimators: {best[0]}, bootstrap: {best[1]} -> test acc: {best[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf92eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-based-methods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
