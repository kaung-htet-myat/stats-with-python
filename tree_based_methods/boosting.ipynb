{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86dc74b",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "In gradient boosting, small weak learners (mostly decision trees) are fitted sequentially. They are trained to predict the residual at training time. Initial prediction is the average value of target variable. Each tree after another tries to correct the initial prediction to get closer and closer to the ground truth of the training set.<br><br>\n",
    "At inference time, prediction of each learner is scaled with learning rate and is added to the initial prediction step by step. Which means that each weak learner gradually corrects the initial prediction to reach to the optimal prediction which is as close as possible to the training ground truth.<br><br>\n",
    "Why don't we just fit one decision tree to correct the initial prediction in one step? Decision trees have high variance and prone to overfitting. By fitting one big enough decision tree to predict the residual in one step, the tree will memorize and overfit the noise of the training set. By using only weak learners at each step, we are only trying to correct the residual of the previous step's prediction just by a little bit and by scaling the predicted residual of current model by learning rate, we are saying that we trust the prediction of current model but only up to a small bit.<br>\n",
    "It also distributes the pattern finding responsibility across multiple trees instead of forcing one big tree to find all the patterns which can lead to overfitting.<br><br>\n",
    "From another perspective, each learner in gradient boosting ensemble is predicting the negative gradient w.r.t to loss value (residual in this case). Each learner predicts the negative gradient and that gradient is scaled with learning rate and used to update the initial prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d454104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45627d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103594, 22) (103594,)\n",
      "(25893, 22) (25893,)\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# drop unneccessary columns\n",
    "df_train = df_train.drop(columns=['Unnamed: 0', 'id'])\n",
    "df_test = df_test.drop(columns=['Unnamed: 0', 'id'])\n",
    "\n",
    "# drop missing values\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# convert Arrival Delay in Minutes to integer\n",
    "df_train['Arrival Delay in Minutes'] = df_train['Arrival Delay in Minutes'].astype(np.int64)\n",
    "df_test['Arrival Delay in Minutes'] = df_test['Arrival Delay in Minutes'].astype(np.int64)\n",
    "\n",
    "# divide categorical and numerical features\n",
    "cat_feats = []\n",
    "num_feats = []\n",
    "\n",
    "for col, dtype in dict(df_train.dtypes).items():\n",
    "    if dtype == np.dtypes.ObjectDType:\n",
    "        cat_feats.append(col)\n",
    "    elif dtype == np.int64:\n",
    "        num_feats.append(col)\n",
    "\n",
    "target = 'satisfaction'\n",
    "cat_feats.remove(target)\n",
    "\n",
    "# prepare train and test sets\n",
    "train_features = cat_feats + num_feats\n",
    "\n",
    "for cf in cat_feats+[target]:\n",
    "    cats = list(df_train[cf].unique())\n",
    "    cats.sort()\n",
    "    encoder = OrdinalEncoder(categories=[cats])\n",
    "    df_train[cf] = encoder.fit_transform(df_train[[cf]]).astype(int)\n",
    "    \n",
    "    cats = list(df_test[cf].unique())\n",
    "    cats.sort()\n",
    "    encoder = OrdinalEncoder(categories=[cats])\n",
    "    df_test[cf] = encoder.fit_transform(df_test[[cf]]).astype(int)\n",
    "       \n",
    "X_train = df_train[train_features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_test = df_test[train_features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "023ef84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9703554259899222\n",
      "\n",
      "test accuracy: 0.9640057158305333\n",
      "\n",
      "confusion matrix:\n",
      "[[14227   301]\n",
      " [  631 10734]]\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "\n",
    "bst = XGBClassifier(eta=0.2, objective='binary:logistic')\n",
    "\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bst.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "\n",
    "print(f\"train accuracy: {accuracy}\\n\")\n",
    "\n",
    "y_pred = bst.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"test accuracy: {accuracy}\\n\")\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085cc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-based-methods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
